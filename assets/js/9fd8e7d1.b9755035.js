"use strict";(globalThis.webpackChunkdazzleduck_website=globalThis.webpackChunkdazzleduck_website||[]).push([[2567],{8154:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"connector/dazzleduck-sql-spark/overview","title":"DazzleDuck SQL Spark Integration","description":"GitHub Repo","source":"@site/docs/connector/dazzleduck-sql-spark/overview.md","sourceDirName":"connector/dazzleduck-sql-spark","slug":"/connector/dazzleduck-sql-spark/overview","permalink":"/dazzleduck-website/docs/connector/dazzleduck-sql-spark/overview","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Overview","sidebar_position":1},"sidebar":"docSidebar","previous":{"title":"Charts","permalink":"/dazzleduck-website/docs/dazzleduck-arrow-js-ui/charts"},"next":{"title":"Installation","permalink":"/dazzleduck-website/docs/connector/dazzleduck-sql-spark/installation"}}');var i=n(4848),a=n(8453);const t={sidebar_label:"Overview",sidebar_position:1},l="DazzleDuck SQL Spark Integration",c={},d=[{value:"Purpose",id:"purpose",level:2},{value:"What This Module Provides",id:"what-this-module-provides",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:2},{value:"Relationship to DazzleDuck SQL Server",id:"relationship-to-dazzleduck-sql-server",level:2},{value:"Typical Use Cases",id:"typical-use-cases",level:2},{value:"Scope &amp; Non-Goals",id:"scope--non-goals",level:2}];function o(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"dazzleduck-sql-spark-integration",children:"DazzleDuck SQL Spark Integration"})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"https://github.com/dazzleduck-web/dazzleduck-sql-spark",children:(0,i.jsx)(r.img,{src:"https://img.shields.io/badge/GitHub-dazzleduck--sql--spark-blue?logo=github",alt:"GitHub Repo"})})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.a,{href:"https://medium.com/@tanejagagan/supercharge-spark-cluster-with-arrow-flight-server-d3854972dab0",children:(0,i.jsx)(r.img,{src:"https://img.shields.io/badge/Medium-Supercharge%20Spark%20Cluster-green?logo=medium",alt:"Medium Blog"})})}),"\n",(0,i.jsxs)(r.p,{children:["The ",(0,i.jsx)(r.strong,{children:"DazzleDuck SQL Spark Integration"})," enables ",(0,i.jsx)(r.strong,{children:"Apache Spark SQL"})," to query data stored in ",(0,i.jsx)(r.strong,{children:"DazzleDuck SQL Server"})," using ",(0,i.jsx)(r.strong,{children:"Apache Arrow Flight SQL"}),". It allows Spark to treat remote DuckDB-backed datasets as native Spark tables, without copying data into Spark-managed storage."]}),"\n",(0,i.jsxs)(r.p,{children:["This module is designed for ",(0,i.jsx)(r.strong,{children:"distributed analytics"}),", where Spark handles execution planning and parallelism, while DazzleDuck acts as a high-performance, Arrow-native storage and query backend."]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsx)(r.p,{children:"This integration exists to:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["Query ",(0,i.jsx)(r.strong,{children:"remote DuckDB / DuckLake / Parquet"})," data directly from Spark"]}),"\n",(0,i.jsx)(r.li,{children:"Avoid data duplication between DuckDB and Spark"}),"\n",(0,i.jsxs)(r.li,{children:["Use ",(0,i.jsx)(r.strong,{children:"Arrow Flight SQL"})," for efficient, columnar, zero-copy data transfer"]}),"\n",(0,i.jsx)(r.li,{children:"Enable Spark-based analytics on top of DazzleDuck-managed warehouses"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:"It is particularly useful when:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Spark is used for large-scale joins or ML pipelines"}),"\n",(0,i.jsx)(r.li,{children:"DuckDB/DazzleDuck is used for ingestion, compaction, and storage"}),"\n",(0,i.jsx)(r.li,{children:"You want Spark and DuckDB to coexist in the same data architecture"}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"what-this-module-provides",children:"What This Module Provides"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["A ",(0,i.jsx)(r.strong,{children:"Spark DataSource V2"})," implementation"]}),"\n",(0,i.jsxs)(r.li,{children:["An ",(0,i.jsx)(r.strong,{children:"Arrow Flight SQL\u2013backed table provider"})]}),"\n",(0,i.jsx)(r.li,{children:"Partition-aware reading for parallel Spark execution"}),"\n",(0,i.jsx)(r.li,{children:"Predicate and aggregation-aware query planning"}),"\n"]}),"\n",(0,i.jsxs)(r.p,{children:["Spark sees DazzleDuck tables as ",(0,i.jsx)(r.strong,{children:"temporary views"})," backed by a remote Arrow RPC source."]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"Spark SQL\n  \u2502\n  \u2502 DataSource V2\n  \u25bc\nArrowRPCTableProvider\n  \u2502\n  \u2502 Arrow Flight SQL (gRPC)\n  \u25bc\nDazzleDuck SQL Server\n  \u2502\n  \u2502 DuckDB / DuckLake\n  \u25bc\nWarehouse (Parquet / DuckLake)\n"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Spark drives query execution"}),"\n",(0,i.jsx)(r.li,{children:"DazzleDuck serves Arrow record batches"}),"\n",(0,i.jsx)(r.li,{children:"Data remains columnar end-to-end"}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"relationship-to-dazzleduck-sql-server",children:"Relationship to DazzleDuck SQL Server"}),"\n",(0,i.jsxs)(r.p,{children:["This project is ",(0,i.jsx)(r.strong,{children:"not part of"})," ",(0,i.jsx)(r.code,{children:"dazzleduck-sql-server"}),", but depends on it at runtime."]}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Component"}),(0,i.jsx)(r.th,{children:"Responsibility"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Spark"}),(0,i.jsx)(r.td,{children:"Distributed execution & scheduling"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"DazzleDuck SQL Server"}),(0,i.jsx)(r.td,{children:"Storage, ingestion, Arrow Flight SQL"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Arrow Flight"}),(0,i.jsx)(r.td,{children:"Transport layer"})]})]})]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"typical-use-cases",children:"Typical Use Cases"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Distributed analytics over Parquet or DuckLake"}),"\n",(0,i.jsx)(r.li,{children:"Spark-based ETL reading from DuckDB-managed data"}),"\n",(0,i.jsx)(r.li,{children:"Federated architectures (DuckDB for storage, Spark for compute)"}),"\n",(0,i.jsx)(r.li,{children:"Large partitioned reads with predicate pushdown"}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h2,{id:"scope--non-goals",children:"Scope & Non-Goals"}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"In scope:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Reading data from DazzleDuck into Spark"}),"\n",(0,i.jsx)(r.li,{children:"Partition-aware parallelism"}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Out of scope:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Writing data back to DazzleDuck from Spark"}),"\n",(0,i.jsx)(r.li,{children:"Replacing Spark\u2019s execution engine"}),"\n",(0,i.jsx)(r.li,{children:"Acting as a Spark catalog"}),"\n"]})]})}function h(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>l});var s=n(6540);const i={},a=s.createContext(i);function t(e){const r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),s.createElement(a.Provider,{value:r},e.children)}}}]);